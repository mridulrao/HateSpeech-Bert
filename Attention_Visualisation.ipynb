{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff2fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa1ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6d7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513b7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3294a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove embeddings \n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "global_vectors = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b36e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>label</th>\n",
       "      <th>extra_label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25462</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>There are not any jew signatures on our Declar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22529</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Hey ,muslim dickhead !!!!!!!! Why don;t you go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29861</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Generally true, but there are indeed missionar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6358</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Why do so many filthy wetback half-breed spic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7907</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Why are girl so ugly to the girls that are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34234</th>\n",
       "      <td>6198</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Why are Hindus always an easy target. On my re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34235</th>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>You people should be proud to be gay,lesbian,b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34236</th>\n",
       "      <td>14780</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>i hate jews so much i wish the holocaust actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34237</th>\n",
       "      <td>8837</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>ayo i even kill handicapped and crippled bitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34238</th>\n",
       "      <td>29714</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>@Legionary1488 @JomavaesJos now we have a Eblo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34239 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id  label extra_label  \\\n",
       "0          25462      1           a   \n",
       "1          22529      1           a   \n",
       "2          29861      0           a   \n",
       "3           6358      1           a   \n",
       "4           7907      0           a   \n",
       "...          ...    ...         ...   \n",
       "34234       6198      0           a   \n",
       "34235       2100      0           a   \n",
       "34236      14780      1           a   \n",
       "34237       8837      1           a   \n",
       "34238      29714      1           a   \n",
       "\n",
       "                                                    text  \n",
       "0      There are not any jew signatures on our Declar...  \n",
       "1      Hey ,muslim dickhead !!!!!!!! Why don;t you go...  \n",
       "2      Generally true, but there are indeed missionar...  \n",
       "3      Why do so many filthy wetback half-breed spic ...  \n",
       "4      Why are girl so ugly to the girls that are jus...  \n",
       "...                                                  ...  \n",
       "34234  Why are Hindus always an easy target. On my re...  \n",
       "34235  You people should be proud to be gay,lesbian,b...  \n",
       "34236  i hate jews so much i wish the holocaust actua...  \n",
       "34237  ayo i even kill handicapped and crippled bitch...  \n",
       "34238  @Legionary1488 @JomavaesJos now we have a Eblo...  \n",
       "\n",
       "[34239 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('data/train.tsv', delimiter='\\t', header=None, names=['unique_id', 'label', 'extra_label', 'text'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15f1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_df.text.values\n",
    "labels = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cabb0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853737f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20886"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set(token for tokens in tokenized_texts for token in tokens)\n",
    "vocab_size = len(unique_tokens)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d640c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 25\n",
    "pad_tokenized_text = [tokens + [\"\"] * (max_words - len(tokens)) if len(tokens) < max_words else tokens[:max_words] for tokens in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca3c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "unk_token = \"<unk>\"\n",
    "unk_index = 0\n",
    "\n",
    "glove_vocab = vocab(global_vectors.stoi)\n",
    "glove_vocab.insert_token(\"<unk>\",unk_index)\n",
    "#this is necessary otherwise it will throw runtime error if OOV token is queried \n",
    "glove_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beae8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tokenized_text = [glove_vocab(tokens) for tokens in pad_tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2f32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tokenized_text = torch.tensor(pad_tokenized_text)\n",
    "labels = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4ffc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "batch = 32\n",
    "embed_len = 100\n",
    "\n",
    "\n",
    "train_data = TensorDataset(pad_tokenized_text, labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e95e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        #column can be anything, more dimension better accuracy\n",
    "        #row must be same as INPUT DIMENSION column\n",
    "        #query matrix\n",
    "        self.Wq = torch.nn.Parameter(torch.randn(input_dim, input_dim)) \n",
    "        #key matrix\n",
    "        self.Wk = torch.nn.Parameter(torch.randn(input_dim, input_dim)) \n",
    "        #value matrix\n",
    "        self.Wv = torch.nn.Parameter(torch.randn(input_dim, input_dim))\n",
    "        \n",
    "        #for normalization/probablity distribution\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.transpose(0, 1) # Assuming x is of shape (sequence_length, batch_size, input_dim)\n",
    "        #print(x.shape)\n",
    "        #print(self.Wq.shape)\n",
    "        queries = torch.matmul(x, self.Wq)\n",
    "        keys = torch.matmul(x, self.Wk)\n",
    "        values = torch.matmul(x, self.Wv)\n",
    "        \n",
    "        #bmm = matrix-matrix product\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        \n",
    "        #normalise score \n",
    "        attention = self.softmax(scores)\n",
    "        \n",
    "        #update attention weight\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        \n",
    "        weighted_transposed = weighted.transpose(0, 1)\n",
    "        \n",
    "        return weighted, weighted_transposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6cd6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customeModel(torch.nn.Module):\n",
    "    def __init__(self, glove_vectors, input_dim, hidden_size, embedding_dimensions, num_classes):\n",
    "        super(customeModel, self).__init__()\n",
    "        \n",
    "        #variables\n",
    "        self.glove_vectors = glove_vectors\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #layer\n",
    "        self.embedding_layer = torch.nn.Embedding.from_pretrained(self.glove_vectors.vectors, freeze=True, sparse=True)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm_1 = torch.nn.LSTM(self.embedding_dimensions, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.attention = SelfAttention(self.input_dim)\n",
    "        \n",
    "        self.lstm_2 = torch.nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, batch_input):\n",
    "        embeds = self.embedding_layer(batch_input)\n",
    "        print(embeds.shape)\n",
    "        lstm_out_1, _ = self.lstm_1(embeds)\n",
    "        print(lstm_out_1.shape)\n",
    "        attn_out, attn_out_transposed = self.attention(lstm_out_1)\n",
    "        lstm_out_2, _ = self.lstm_2(attn_out_transposed)\n",
    "        output = self.output_layer(lstm_out_2) #lstm_out_2[:, -1, :] using last hidden_state for classification\n",
    "        output_sigmoid = self.sigmoid(output)\n",
    "        \n",
    "        #return F.log_softmax(output, dim=1)\n",
    "        return output_sigmoid, attn_out, attn_out_transposed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe03540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "custom_model = customeModel(glove_vectors = global_vectors, #glove vectors\n",
    "                            input_dim = 25, #self-attention\n",
    "                            hidden_size = 25, #lstm cell\n",
    "                            embedding_dimensions = 100, #glove embedding \n",
    "                            num_classes = 1) #output classes are 2 but neurons req out_class - 1\n",
    "\n",
    "optimizer = Adam(custom_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "383decd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f7153c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 25])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2222c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 25, 100])\n",
      "torch.Size([31, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs, attn_out, attn_out_trans = custom_model(b_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b6c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:14<00:58,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0836, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [00:21<00:51,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.1173, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [00:29<00:43,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0118, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [00:36<00:36,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0186, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 6/10 [00:43<00:28,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████▊             | 7/10 [00:50<00:21,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0639, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████▏        | 8/10 [00:57<00:14,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0135, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████▌    | 9/10 [01:04<00:07,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0197, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:11<00:00,  7.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "for epochs in tqdm(range(10)):\n",
    "    if epochs > 1:\n",
    "        print(\"Loss \" + str(loss))\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        \n",
    "        #clear gradients \n",
    "        custom_model.zero_grad()\n",
    "        \n",
    "        #get output and transform it according to labels \n",
    "        outputs, attn_out, attn_out_trans = custom_model(b_input_ids)\n",
    "        class_probs = torch.mean(outputs, dim=0)\n",
    "        \n",
    "        #reshape labels and convert to float \n",
    "        b_labels_2d = b_labels.view(-1, 1)\n",
    "        b_labels_2d = b_labels_2d.float()\n",
    "        \n",
    "        loss = loss_function(class_probs, b_labels_2d)\n",
    "        #print(\"Loss \" + str(loss))\n",
    "        #compute gradient \n",
    "        loss.backward()\n",
    "        \n",
    "        #update parameters \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083f6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 25\n",
    "\n",
    "def pad_sentence(sentence):\n",
    "    if len(sentence) < max_words:\n",
    "        \n",
    "        sentence = sentence + [\"\"]*(max_words - len(sentence))\n",
    "        \n",
    "    else:\n",
    "        sentence = sentence[:max_words]\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57bd4c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'this', 'bitch', '.', 'this', 'sentence', 'should', 'be', 'offensive', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "tensor([[0.9820]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Fuck this bitch. This sentence should be offensive.'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_weight, attn_weight_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "print(torch.mean(output, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3d033e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 25])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da9f095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 25])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4e0cd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.squeeze(attn_weight)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0844c6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  81.8591,  -15.5634,  -27.9860,  -69.1191,  -55.4261,   14.0503,\n",
       "          18.0048,   90.8945,   88.3963,    4.1183,  -16.9858,   13.9386,\n",
       "         -82.0713,  -11.4519,   26.5646,    4.4011,  -47.4977,  -99.7784,\n",
       "         -12.6317,   60.4945,  109.4932,   62.3862,   11.9425,   30.0096,\n",
       "        -149.0325], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d113ae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'nigger', 'tried', 'to', 'kill', 'me', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "tensor([[0.9995]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'This nigger tried to kill me'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_weight, attn_weight_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "print(torch.mean(output, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f5ecc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.squeeze(attn_weight)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fec00a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  81.8591,  -15.5634,  -27.9860,  -69.1191,  -55.4261,   14.0503,\n",
       "          18.0048,   90.8945,   88.3963,    4.1183,  -16.9858,   13.9386,\n",
       "         -82.0713,  -11.4519,   26.5646,    4.4011,  -47.4977,  -99.7784,\n",
       "         -12.6317,   60.4945,  109.4932,   62.3862,   11.9425,   30.0096,\n",
       "        -149.0325], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64ef9df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 17, 12,  3,  4, 16,  2, 10,  1, 18, 13,  9, 15, 22, 11,  5,  6,\n",
       "       14, 23, 19, 21,  0,  8,  7, 20])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argsort(attn_weight.sum(dim=0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5d8b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "attn_scores_softmax = softmax(attn_weight, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bec9ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2077e-01, 2.6530e-03, 1.4861e-03, 2.9425e-04, 5.0488e-04, 8.0333e-03,\n",
       "         9.8725e-03, 1.7081e-01, 1.4336e-01, 5.2988e-03, 2.2640e-03, 8.6876e-03,\n",
       "         1.7485e-04, 2.8332e-03, 1.2584e-02, 5.7206e-03, 6.5695e-04, 8.4294e-05,\n",
       "         2.8274e-03, 5.1416e-02, 3.6953e-01, 5.7315e-02, 7.6154e-03, 1.5198e-02,\n",
       "         1.2822e-05],\n",
       "        [1.1957e-01, 2.6194e-03, 1.4293e-03, 2.8701e-04, 4.7210e-04, 8.0463e-03,\n",
       "         9.6556e-03, 1.7050e-01, 1.3372e-01, 5.1828e-03, 2.1844e-03, 9.2473e-03,\n",
       "         1.7254e-04, 2.6864e-03, 1.1988e-02, 5.9125e-03, 5.9815e-04, 7.8831e-05,\n",
       "         2.8811e-03, 5.2453e-02, 3.8119e-01, 5.6683e-02, 7.5647e-03, 1.4858e-02,\n",
       "         1.2571e-05],\n",
       "        [1.1753e-01, 2.4600e-03, 1.4197e-03, 2.8520e-04, 4.8155e-04, 7.8401e-03,\n",
       "         9.1457e-03, 1.7257e-01, 1.4909e-01, 5.2171e-03, 2.2865e-03, 8.0245e-03,\n",
       "         1.7402e-04, 2.7122e-03, 1.2892e-02, 5.5837e-03, 6.4062e-04, 8.0870e-05,\n",
       "         2.8089e-03, 5.2202e-02, 3.6867e-01, 5.5647e-02, 7.3959e-03, 1.4824e-02,\n",
       "         1.1688e-05],\n",
       "        [1.0770e-01, 1.5899e-03, 1.3847e-03, 2.5185e-04, 4.8044e-04, 6.8387e-03,\n",
       "         6.6714e-03, 1.7308e-01, 2.4937e-01, 5.3540e-03, 2.6285e-03, 4.0524e-03,\n",
       "         1.5799e-04, 2.9240e-03, 1.7307e-02, 3.8195e-03, 8.6670e-04, 8.6042e-05,\n",
       "         2.2295e-03, 4.6971e-02, 3.0090e-01, 4.5151e-02, 5.9850e-03, 1.4194e-02,\n",
       "         7.3036e-06],\n",
       "        [8.7556e-02, 9.2085e-04, 1.0547e-03, 2.0036e-04, 3.5866e-04, 6.2546e-03,\n",
       "         3.9796e-03, 1.7460e-01, 3.2158e-01, 4.6382e-03, 2.5523e-03, 2.2424e-03,\n",
       "         1.4318e-04, 2.2519e-03, 1.8929e-02, 3.1051e-03, 7.6078e-04, 6.6164e-05,\n",
       "         2.1615e-03, 5.0781e-02, 2.6239e-01, 3.7507e-02, 4.6067e-03, 1.1354e-02,\n",
       "         3.9818e-06],\n",
       "        [1.2246e-01, 2.2574e-03, 1.5602e-03, 2.7863e-04, 5.0317e-04, 7.7110e-03,\n",
       "         9.1991e-03, 1.7012e-01, 1.7431e-01, 5.5964e-03, 2.3731e-03, 7.3026e-03,\n",
       "         1.6093e-04, 3.1400e-03, 1.4003e-02, 4.8417e-03, 7.5712e-04, 8.8049e-05,\n",
       "         2.4555e-03, 4.7339e-02, 3.5005e-01, 5.0975e-02, 6.9912e-03, 1.5513e-02,\n",
       "         1.0855e-05],\n",
       "        [1.2043e-01, 2.4114e-03, 1.4870e-03, 2.8344e-04, 4.9013e-04, 7.8682e-03,\n",
       "         9.3177e-03, 1.7124e-01, 1.5581e-01, 5.3813e-03, 2.2998e-03, 7.9839e-03,\n",
       "         1.6806e-04, 2.8943e-03, 1.3147e-02, 5.3316e-03, 6.7996e-04, 8.3684e-05,\n",
       "         2.6747e-03, 5.0192e-02, 3.6343e-01, 5.3972e-02, 7.2679e-03, 1.5149e-02,\n",
       "         1.1561e-05],\n",
       "        [1.1969e-01, 2.5097e-03, 1.4503e-03, 2.8508e-04, 4.7891e-04, 7.9843e-03,\n",
       "         9.4425e-03, 1.7122e-01, 1.4387e-01, 5.2627e-03, 2.2362e-03, 8.5923e-03,\n",
       "         1.7074e-04, 2.7670e-03, 1.2519e-02, 5.6486e-03, 6.3251e-04, 8.0741e-05,\n",
       "         2.7993e-03, 5.1692e-02, 3.7278e-01, 5.5517e-02, 7.4166e-03, 1.4943e-02,\n",
       "         1.2031e-05],\n",
       "        [1.1971e-01, 2.5348e-03, 1.4469e-03, 2.8565e-04, 4.7773e-04, 8.0119e-03,\n",
       "         9.4962e-03, 1.7110e-01, 1.4159e-01, 5.2460e-03, 2.2236e-03, 8.7365e-03,\n",
       "         1.7113e-04, 2.7511e-03, 1.2395e-02, 5.7098e-03, 6.2508e-04, 8.0359e-05,\n",
       "         2.8208e-03, 5.1897e-02, 3.7447e-01, 5.5832e-02, 7.4499e-03, 1.4923e-02,\n",
       "         1.2161e-05],\n",
       "        [1.1993e-01, 2.5474e-03, 1.4518e-03, 2.8627e-04, 4.7949e-04, 8.0251e-03,\n",
       "         9.5462e-03, 1.7100e-01, 1.4135e-01, 5.2529e-03, 2.2229e-03, 8.7749e-03,\n",
       "         1.7116e-04, 2.7607e-03, 1.2381e-02, 5.7166e-03, 6.2668e-04, 8.0629e-05,\n",
       "         2.8214e-03, 5.1831e-02, 3.7437e-01, 5.5942e-02, 7.4639e-03, 1.4948e-02,\n",
       "         1.2235e-05],\n",
       "        [1.2025e-01, 2.5598e-03, 1.4601e-03, 2.8709e-04, 4.8260e-04, 8.0373e-03,\n",
       "         9.6067e-03, 1.7090e-01, 1.4163e-01, 5.2675e-03, 2.2257e-03, 8.7887e-03,\n",
       "         1.7114e-04, 2.7796e-03, 1.2396e-02, 5.7101e-03, 6.3128e-04, 8.1156e-05,\n",
       "         2.8168e-03, 5.1681e-02, 3.7372e-01, 5.6034e-02, 7.4766e-03, 1.4993e-02,\n",
       "         1.2313e-05],\n",
       "        [1.2062e-01, 2.5736e-03, 1.4704e-03, 2.8811e-04, 4.8662e-04, 8.0508e-03,\n",
       "         9.6767e-03, 1.7079e-01, 1.4216e-01, 5.2858e-03, 2.2303e-03, 8.7917e-03,\n",
       "         1.7114e-04, 2.8034e-03, 1.2425e-02, 5.6987e-03, 6.3753e-04, 8.1841e-05,\n",
       "         2.8108e-03, 5.1497e-02, 3.7275e-01, 5.6144e-02, 7.4907e-03, 1.5047e-02,\n",
       "         1.2402e-05],\n",
       "        [1.2101e-01, 2.5880e-03, 1.4816e-03, 2.8924e-04, 4.9118e-04, 8.0652e-03,\n",
       "         9.7505e-03, 1.7069e-01, 1.4284e-01, 5.3057e-03, 2.2360e-03, 8.7856e-03,\n",
       "         1.7118e-04, 2.8296e-03, 1.2463e-02, 5.6851e-03, 6.4474e-04, 8.2614e-05,\n",
       "         2.8044e-03, 5.1304e-02, 3.7158e-01, 5.6272e-02, 7.5056e-03, 1.5106e-02,\n",
       "         1.2496e-05],\n",
       "        [1.2138e-01, 2.6016e-03, 1.4928e-03, 2.9037e-04, 4.9587e-04, 8.0793e-03,\n",
       "         9.8214e-03, 1.7062e-01, 1.4362e-01, 5.3255e-03, 2.2422e-03, 8.7710e-03,\n",
       "         1.7124e-04, 2.8560e-03, 1.2505e-02, 5.6700e-03, 6.5228e-04, 8.3407e-05,\n",
       "         2.7982e-03, 5.1117e-02, 3.7032e-01, 5.6401e-02, 7.5196e-03, 1.5164e-02,\n",
       "         1.2585e-05],\n",
       "        [1.2170e-01, 2.6129e-03, 1.5031e-03, 2.9141e-04, 5.0028e-04, 8.0916e-03,\n",
       "         9.8834e-03, 1.7056e-01, 1.4444e-01, 5.3439e-03, 2.2485e-03, 8.7492e-03,\n",
       "         1.7130e-04, 2.8808e-03, 1.2550e-02, 5.6542e-03, 6.5959e-04, 8.4155e-05,\n",
       "         2.7923e-03, 5.0946e-02, 3.6906e-01, 5.6517e-02, 7.5311e-03, 1.5217e-02,\n",
       "         1.2661e-05],\n",
       "        [1.2197e-01, 2.6213e-03, 1.5120e-03, 2.9227e-04, 5.0415e-04, 8.1014e-03,\n",
       "         9.9332e-03, 1.7052e-01, 1.4527e-01, 5.3601e-03, 2.2544e-03, 8.7222e-03,\n",
       "         1.7134e-04, 2.9026e-03, 1.2594e-02, 5.6380e-03, 6.6627e-04, 8.4816e-05,\n",
       "         2.7865e-03, 5.0796e-02, 3.6788e-01, 5.6605e-02, 7.5392e-03, 1.5262e-02,\n",
       "         1.2720e-05],\n",
       "        [1.2218e-01, 2.6265e-03, 1.5194e-03, 2.9293e-04, 5.0734e-04, 8.1082e-03,\n",
       "         9.9699e-03, 1.7049e-01, 1.4607e-01, 5.3740e-03, 2.2599e-03, 8.6918e-03,\n",
       "         1.7135e-04, 2.9212e-03, 1.2636e-02, 5.6219e-03, 6.7212e-04, 8.5373e-05,\n",
       "         2.7810e-03, 5.0669e-02, 3.6683e-01, 5.6661e-02, 7.5437e-03, 1.5299e-02,\n",
       "         1.2759e-05],\n",
       "        [1.2234e-01, 2.6289e-03, 1.5252e-03, 2.9340e-04, 5.0987e-04, 8.1124e-03,\n",
       "         9.9944e-03, 1.7049e-01, 1.4683e-01, 5.3854e-03, 2.2647e-03, 8.6596e-03,\n",
       "         1.7133e-04, 2.9364e-03, 1.2676e-02, 5.6061e-03, 6.7709e-04, 8.5823e-05,\n",
       "         2.7757e-03, 5.0562e-02, 3.6591e-01, 5.6688e-02, 7.5449e-03, 1.5327e-02,\n",
       "         1.2782e-05],\n",
       "        [1.2246e-01, 2.6289e-03, 1.5298e-03, 2.9370e-04, 5.1180e-04, 8.1144e-03,\n",
       "         1.0008e-02, 1.7049e-01, 1.4753e-01, 5.3948e-03, 2.2689e-03, 8.6267e-03,\n",
       "         1.7129e-04, 2.9486e-03, 1.2713e-02, 5.5909e-03, 6.8124e-04, 8.6178e-05,\n",
       "         2.7707e-03, 5.0474e-02, 3.6512e-01, 5.6689e-02, 7.5434e-03, 1.5349e-02,\n",
       "         1.2789e-05],\n",
       "        [1.2253e-01, 2.6269e-03, 1.5332e-03, 2.9387e-04, 5.1322e-04, 8.1146e-03,\n",
       "         1.0014e-02, 1.7050e-01, 1.4819e-01, 5.4024e-03, 2.2726e-03, 8.5939e-03,\n",
       "         1.7123e-04, 2.9583e-03, 1.2746e-02, 5.5764e-03, 6.8468e-04, 8.6450e-05,\n",
       "         2.7661e-03, 5.0402e-02, 3.6444e-01, 5.6670e-02, 7.5399e-03, 1.5365e-02,\n",
       "         1.2784e-05],\n",
       "        [1.2258e-01, 2.6236e-03, 1.5356e-03, 2.9393e-04, 5.1422e-04, 8.1135e-03,\n",
       "         1.0013e-02, 1.7051e-01, 1.4879e-01, 5.4084e-03, 2.2759e-03, 8.5616e-03,\n",
       "         1.7115e-04, 2.9658e-03, 1.2777e-02, 5.5627e-03, 6.8749e-04, 8.6654e-05,\n",
       "         2.7617e-03, 5.0344e-02, 3.6386e-01, 5.6636e-02, 7.5347e-03, 1.5375e-02,\n",
       "         1.2771e-05],\n",
       "        [1.2259e-01, 2.6192e-03, 1.5373e-03, 2.9390e-04, 5.1487e-04, 8.1113e-03,\n",
       "         1.0007e-02, 1.7054e-01, 1.4935e-01, 5.4131e-03, 2.2786e-03, 8.5303e-03,\n",
       "         1.7107e-04, 2.9714e-03, 1.2805e-02, 5.5499e-03, 6.8976e-04, 8.6801e-05,\n",
       "         2.7577e-03, 5.0298e-02, 3.6337e-01, 5.6592e-02, 7.5284e-03, 1.5382e-02,\n",
       "         1.2750e-05],\n",
       "        [1.2259e-01, 2.6140e-03, 1.5383e-03, 2.9381e-04, 5.1524e-04, 8.1085e-03,\n",
       "         9.9966e-03, 1.7056e-01, 1.4986e-01, 5.4166e-03, 2.2810e-03, 8.5000e-03,\n",
       "         1.7099e-04, 2.9755e-03, 1.2830e-02, 5.5379e-03, 6.9159e-04, 8.6900e-05,\n",
       "         2.7541e-03, 5.0262e-02, 3.6295e-01, 5.6539e-02, 7.5213e-03, 1.5386e-02,\n",
       "         1.2725e-05],\n",
       "        [1.2258e-01, 2.6083e-03, 1.5388e-03, 2.9367e-04, 5.1538e-04, 8.1051e-03,\n",
       "         9.9831e-03, 1.7059e-01, 1.5033e-01, 5.4192e-03, 2.2831e-03, 8.4709e-03,\n",
       "         1.7090e-04, 2.9783e-03, 1.2853e-02, 5.5269e-03, 6.9302e-04, 8.6960e-05,\n",
       "         2.7508e-03, 5.0236e-02, 3.6259e-01, 5.6482e-02, 7.5136e-03, 1.5386e-02,\n",
       "         1.2695e-05],\n",
       "        [1.2254e-01, 2.6021e-03, 1.5389e-03, 2.9348e-04, 5.1533e-04, 8.1013e-03,\n",
       "         9.9671e-03, 1.7063e-01, 1.5076e-01, 5.4210e-03, 2.2849e-03, 8.4431e-03,\n",
       "         1.7081e-04, 2.9800e-03, 1.2874e-02, 5.5166e-03, 6.9412e-04, 8.6987e-05,\n",
       "         2.7478e-03, 5.0217e-02, 3.6229e-01, 5.6421e-02, 7.5056e-03, 1.5384e-02,\n",
       "         1.2663e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a229e89",
   "metadata": {},
   "source": [
    "# From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c4b2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        #column can be anything, more dimension better accuracy\n",
    "        #row must be same as INPUT DIMENSION column\n",
    "        #query matrix\n",
    "        self.Wq = torch.nn.Linear(input_dim, input_dim) \n",
    "        #key matrix\n",
    "        self.Wk = torch.nn.Linear(input_dim, input_dim) \n",
    "        #value matrix\n",
    "        self.Wv = torch.nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        #for normalization/probablity distribution\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.transpose(0, 1) # Assuming x is of shape (sequence_length, batch_size, input_dim)\n",
    "        #print(x.shape)\n",
    "        #print(self.Wq.shape)\n",
    "        \n",
    "        queries = self.Wq(x)\n",
    "        keys = self.Wk(x)\n",
    "        values = self.Wv(x)\n",
    "        \n",
    "        print(\"Queries  \" + str(queries.shape))\n",
    "        print(\"Keys  \" + str(keys.shape))\n",
    "        print(\"Values  \" + str(values.shape))\n",
    "        \n",
    "        #bmm = matrix-matrix product\n",
    "        #scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.input_dim, dtype=torch.float32))\n",
    "        \n",
    "        #normalise score \n",
    "        #attention = self.softmax(scores)\n",
    "        attention_weights = self.softmax(scores)\n",
    "        print(\"Scores \" + str(attention_weights.shape))\n",
    "        \n",
    "        #update attention weight\n",
    "        weighted = torch.matmul(attention_weights, values)\n",
    "        print(\"Weighted \" + str(weighted.shape))\n",
    "        \n",
    "        weighted_transposed = weighted.transpose(0, 1)\n",
    "        print(\"Weighted Transpose \" + str(weighted_transposed.shape))\n",
    "        \n",
    "        return attention_weights, weighted, weighted_transposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ab6ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customeModel(torch.nn.Module):\n",
    "    def __init__(self, glove_vectors, input_dim, hidden_size, embedding_dimensions, num_classes):\n",
    "        super(customeModel, self).__init__()\n",
    "        \n",
    "        #variables\n",
    "        self.glove_vectors = glove_vectors\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #layer\n",
    "        self.embedding_layer = torch.nn.Embedding.from_pretrained(self.glove_vectors.vectors, freeze=True, sparse=True)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm_1 = torch.nn.LSTM(self.embedding_dimensions, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.attention = SelfAttention(self.input_dim)\n",
    "        \n",
    "        self.lstm_2 = torch.nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, batch_input):\n",
    "        embeds = self.embedding_layer(batch_input)\n",
    "        print(\"Embeds \" + str(embeds.shape))\n",
    "        lstm_out_1, _ = self.lstm_1(embeds)\n",
    "        print(\"LSTM 1 + \" + str(lstm_out_1.shape))\n",
    "        attn_weights, weighted_output, weighted_output_transposed = self.attention(lstm_out_1)\n",
    "        print(\"Attn wt \"  + str(attn_weights.shape))\n",
    "        print(\"Weighted Output \"  + str(weighted_output.shape))\n",
    "        print(\"Weighted Output Transposed \"  + str(weighted_output_transposed.shape))\n",
    "        lstm_out_2, _ = self.lstm_2(weighted_output)\n",
    "        output = self.output_layer(lstm_out_2) #lstm_out_2[:, -1, :] using last hidden_state for classification\n",
    "        output_sigmoid = self.sigmoid(output)\n",
    "        \n",
    "        #return F.log_softmax(output, dim=1)\n",
    "        return output_sigmoid, attn_weights, weighted_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6c273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "custom_model = customeModel(glove_vectors = global_vectors, #glove vectors\n",
    "                            input_dim = 25, #self-attention\n",
    "                            hidden_size = 25, #lstm cell\n",
    "                            embedding_dimensions = 100, #glove embedding \n",
    "                            num_classes = 1) #output classes are 2 but neurons req out_class - 1\n",
    "\n",
    "optimizer = Adam(custom_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6415d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeds torch.Size([31, 25, 100])\n",
      "LSTM 1 + torch.Size([31, 25, 25])\n",
      "Queries  torch.Size([31, 25, 25])\n",
      "Keys  torch.Size([31, 25, 25])\n",
      "Values  torch.Size([31, 25, 25])\n",
      "Scores torch.Size([31, 25, 25])\n",
      "Weighted torch.Size([31, 25, 25])\n",
      "Weighted Transpose torch.Size([25, 31, 25])\n",
      "Attn wt torch.Size([31, 25, 25])\n",
      "Weighted Output torch.Size([31, 25, 25])\n",
      "Weighted Output Transposed torch.Size([25, 31, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs, attn_wt, wt_out = custom_model(b_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19521108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 25, 25])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "295b8218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 25, 25])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "687af54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(outputs, dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc370522",
   "metadata": {},
   "source": [
    "# Train Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0978eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        #column can be anything, more dimension better accuracy\n",
    "        #row must be same as INPUT DIMENSION column\n",
    "        #query matrix\n",
    "        self.Wq = torch.nn.Linear(input_dim, input_dim) \n",
    "        #key matrix\n",
    "        self.Wk = torch.nn.Linear(input_dim, input_dim) \n",
    "        #value matrix\n",
    "        self.Wv = torch.nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        #for normalization/probablity distribution\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.transpose(0, 1) # Assuming x is of shape (sequence_length, batch_size, input_dim)\n",
    "        #print(x.shape)\n",
    "        #print(self.Wq.shape)\n",
    "        \n",
    "        queries = self.Wq(x)\n",
    "        keys = self.Wk(x)\n",
    "        values = self.Wv(x)\n",
    "        \n",
    "        #bmm = matrix-matrix product\n",
    "        #scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.input_dim, dtype=torch.float32))\n",
    "        \n",
    "        #normalise score \n",
    "        #attention = self.softmax(scores)\n",
    "        attention_weights = self.softmax(scores)\n",
    "        #print(\"Scores \" + str(attention_weights.shape))\n",
    "        \n",
    "        #update attention weight\n",
    "        weighted = torch.matmul(attention_weights, values)\n",
    "        #print(\"Weighted \" + str(weighted.shape))\n",
    "        \n",
    "        weighted_transposed = weighted.transpose(0, 1)\n",
    "        #print(\"Weighted Transpose \" + str(weighted_transposed.shape))\n",
    "        \n",
    "        return attention_weights, weighted, weighted_transposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "235561f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customeModel(torch.nn.Module):\n",
    "    def __init__(self, glove_vectors, input_dim, hidden_size, embedding_dimensions, num_classes):\n",
    "        super(customeModel, self).__init__()\n",
    "        \n",
    "        #variables\n",
    "        self.glove_vectors = glove_vectors\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #layer\n",
    "        self.embedding_layer = torch.nn.Embedding.from_pretrained(self.glove_vectors.vectors, freeze=True, sparse=True)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm_1 = torch.nn.LSTM(self.embedding_dimensions, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.attention = SelfAttention(self.input_dim)\n",
    "        \n",
    "        self.lstm_2 = torch.nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True)\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, batch_input):\n",
    "        embeds = self.embedding_layer(batch_input)\n",
    "        #print(\"Embeds \" + str(embeds.shape))\n",
    "        lstm_out_1, _ = self.lstm_1(embeds)\n",
    "        #print(\"LSTM 1 + \" + str(lstm_out_1.shape))\n",
    "        attn_weights, weighted_output, weighted_output_transposed = self.attention(lstm_out_1)\n",
    "        #print(\"Attn wt \"  + str(attn_weights.shape))\n",
    "        #print(\"Weighted Output \"  + str(weighted_output.shape))\n",
    "        #print(\"Weighted Output Transposed \"  + str(weighted_output_transposed.shape))\n",
    "        lstm_out_2, _ = self.lstm_2(weighted_output)\n",
    "        output = self.output_layer(lstm_out_2) #lstm_out_2[:, -1, :] using last hidden_state for classification\n",
    "        output_sigmoid = self.sigmoid(output)\n",
    "        \n",
    "        #return F.log_softmax(output, dim=1)\n",
    "        return output_sigmoid, attn_weights, weighted_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "774fe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "custom_model = customeModel(glove_vectors = global_vectors, #glove vectors\n",
    "                            input_dim = 25, #self-attention\n",
    "                            hidden_size = 25, #lstm cell\n",
    "                            embedding_dimensions = 100, #glove embedding \n",
    "                            num_classes = 1) #output classes are 2 but neurons req out_class - 1\n",
    "\n",
    "optimizer = Adam(custom_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f3dad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:13<00:54,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0731, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [00:20<00:47,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0217, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [00:27<00:41,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0686, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [00:34<00:34,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0095, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 6/10 [00:41<00:27,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0216, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████▊             | 7/10 [00:48<00:20,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0053, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████▏        | 8/10 [00:55<00:13,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████▌    | 9/10 [01:01<00:06,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0017, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:08<00:00,  6.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "for epochs in tqdm(range(10)):\n",
    "    if epochs > 1:\n",
    "        print(\"Loss \" + str(loss))\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        \n",
    "        #clear gradients \n",
    "        custom_model.zero_grad()\n",
    "        \n",
    "        #get output and transform it according to labels \n",
    "        outputs, attn_out, attn_out_trans = custom_model(b_input_ids)\n",
    "        class_probs = torch.mean(outputs, dim=1)\n",
    "        \n",
    "        #reshape labels and convert to float \n",
    "        b_labels_2d = b_labels.view(-1, 1)\n",
    "        b_labels_2d = b_labels_2d.float()\n",
    "        \n",
    "        loss = loss_function(class_probs, b_labels_2d)\n",
    "        #print(\"Loss \" + str(loss))\n",
    "        #compute gradient \n",
    "        loss.backward()\n",
    "        \n",
    "        #update parameters \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59cae06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'this', 'bitch', '.', 'this', 'sentence', 'should', 'be', 'offensive', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "tensor([[0.9957]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = 'Fuck this bitch. This sentence should be offensive.'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "print(torch.mean(output, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "88cecbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 25])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ababf1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out = attn_out.squeeze()\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2033cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6792,  0.6173, 13.7180,  1.5444,  0.1918,  0.2316,  0.0788,  0.0442,\n",
       "         0.4992,  0.0846,  0.1362,  0.1795,  0.1809,  0.1549,  0.1219,  0.0943,\n",
       "         0.0752,  0.0629,  0.0552,  0.0499,  0.0461,  0.0428,  0.0399,  0.0370,\n",
       "         0.0343], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b2fac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'nigger', 'tried', 'to', 'kill', 'me', '.', 'this', 'sentence', 'should', 'be', 'offensive', '.', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "tensor([[0.9998]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = 'This nigger tried to kill me. This sentence should be offensive.'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "print(torch.mean(output, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9d03ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out = attn_out.squeeze()\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c42c2853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0587, 5.8318, 2.7673, 1.6082, 3.5135, 5.7211, 2.0190, 0.3526, 0.4417,\n",
       "        0.1163, 0.0391, 0.4026, 0.0880, 0.1672, 0.2640, 0.3274, 0.3336, 0.2919,\n",
       "        0.2255, 0.1570, 0.1024, 0.0663, 0.0453, 0.0333, 0.0263],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2571d571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bastard', 'tried', 'to', 'kill', 'me', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "tensor([[0.9999]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = 'Bastard tried to kill me'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "print(torch.mean(output, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e4bf6d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out = attn_out.squeeze()\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef2ed958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9454, 0.5490, 0.3616, 1.4496, 2.9873, 2.5174, 2.3567, 2.8675, 3.2261,\n",
       "        2.8674, 2.0675, 1.2733, 0.6942, 0.3481, 0.1742, 0.0953, 0.0589, 0.0404,\n",
       "        0.0297, 0.0229, 0.0184, 0.0152, 0.0129, 0.0112, 0.0099],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9ccdf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13,  2,  1, 12,  0, 11,\n",
       "        3, 10,  6,  5,  9,  7,  4,  8])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(attn_out.sum(axis=0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5429a69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bastard',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'kill',\n",
       " 'me',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1198c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'nigger', 'tried', 'to', 'kill', 'me', '.', 'this', 'sentence', 'should', 'be', 'offensive', '.', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[6 2 4 5 1]\n",
      ".\n",
      "tried\n",
      "kill\n",
      "me\n",
      "nigger\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = 'This nigger tried to kill me. This sentence should be offensive.'\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "#print(torch.mean(output, dim=1))\n",
    "\n",
    "attn_out = attn_out.squeeze()\n",
    "attn_out = attn_out.sum(axis = 0)\n",
    "words = np.argsort(attn_out.detach().numpy())[-5:]\n",
    "\n",
    "print(words)\n",
    "\n",
    "for word in words:\n",
    "    print(padded[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc05bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'transgender', 'but', 'i', \"'\", 'm', 'also', 'mentally', 'ill', 'but', 'it', \"'\", 's', 'still', 'appreciated', '.', '', '', '', '', '', '', '', '']\n",
      "[1 0 2 8 9]\n",
      "am\n",
      "i\n",
      "transgender\n",
      "mentally\n",
      "ill\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = \"I am transgender but I'm also mentally ill but it's still appreciated.\"\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "#print(torch.mean(output, dim=1))\n",
    "\n",
    "attn_out = attn_out.squeeze()\n",
    "attn_out = attn_out.sum(axis = 0)\n",
    "words = np.argsort(attn_out.detach().numpy())[-5:]\n",
    "\n",
    "print(words)\n",
    "\n",
    "for word in words:\n",
    "    print(padded[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5de5c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'transgender', 'but', 'i', \"'\", 'm', 'also', 'mentally', 'ill', 'but', 'it', \"'\", 's', 'still', 'appreciated', '.', '', '', '', '', '', '', '', '']\n",
      "0.00047157696\n",
      "[1 0 2 8 9]\n",
      "am\n",
      "i\n",
      "transgender\n",
      "mentally\n",
      "ill\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_sentence = \"I am transgender but I'm also mentally ill but it's still appreciated.\"\n",
    "tokenized_test_sentence = tokenizer(test_sentence)\n",
    "\n",
    "#pad\n",
    "pad_tokenized_test_sentence = pad_sentence(tokenized_test_sentence)\n",
    "padded = pad_tokenized_test_sentence\n",
    "print(pad_tokenized_test_sentence)\n",
    "\n",
    "#word - index\n",
    "pad_tokenized_test_sentence = glove_vocab(pad_tokenized_test_sentence)\n",
    "\n",
    "#tensor\n",
    "pad_tokenized_test_sentence = torch.tensor(pad_tokenized_test_sentence)\n",
    "\n",
    "#shape\n",
    "pad_tokenized_test_sentence = pad_tokenized_test_sentence.view(1, -1)\n",
    "pad_tokenized_test_sentence.shape\n",
    "\n",
    "custom_model.eval()\n",
    "output, attn_out, attn_out_trans = custom_model(pad_tokenized_test_sentence)\n",
    "\n",
    "output = torch.mean(output, dim=1).detach().numpy()\n",
    "print(output[0][0])\n",
    "\n",
    "attn_out = attn_out.squeeze()\n",
    "attn_out = attn_out.sum(axis = 0)\n",
    "words = np.argsort(attn_out.detach().numpy())[-5:]\n",
    "\n",
    "print(words)\n",
    "\n",
    "for word in words:\n",
    "    print(padded[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4fd98c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_identify(text):\n",
    "    tokenized_sentence = tokenizer(text)\n",
    "    pad_tokenized_sentence = pad_sentence(tokenized_sentence)\n",
    "    padded = pad_tokenized_sentence\n",
    "    pad_tokenized_sentence = glove_vocab(pad_tokenized_sentence)\n",
    "    pad_tokenized_sentence = torch.tensor(pad_tokenized_sentence)\n",
    "    pad_tokenized_sentence = pad_tokenized_sentence.view(1, -1)\n",
    "    custom_model.eval()\n",
    "    output, attn_out, attn_out_trans = custom_model(pad_tokenized_sentence)\n",
    "    output = torch.mean(output, dim=1).detach().numpy()\n",
    "    \n",
    "    label = 'non-offensive'\n",
    "    if output[0][0] > 0.6:\n",
    "        label = 'offensive'\n",
    "        \n",
    "    attn_out = attn_out.squeeze()\n",
    "    attn_out = attn_out.sum(axis = 0)\n",
    "    words = np.argsort(attn_out.detach().numpy())[-5:]\n",
    "    \n",
    "    words_list = []\n",
    "    \n",
    "    for word in words:\n",
    "        words_list.append(padded[word])\n",
    "        \n",
    "    return label, words_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12b6dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('non-offensive', ['am', 'i', 'transgender', 'mentally', 'ill'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_identify(\"I am transgender but I'm also mentally ill but it's still appreciated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22102b61",
   "metadata": {},
   "source": [
    "# Test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c2be1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JEW Get the fuck out of here you jewish son of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is how I too feel about #LGBTQ #Pride It'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You're right, I didn't think about that. And w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am transgender but I'm also mentally ill but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump is the only jews true friend. I hope tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8642</th>\n",
       "      <td>Hitler didn't finish it. Can u. If a nigger ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8643</th>\n",
       "      <td>Ah yeah, I'm actually Dominican myself! And mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8644</th>\n",
       "      <td>\"@iamkrause: No need to thank me, killing nigg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>+Kinda Linda yea I know I just wish people wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8646</th>\n",
       "      <td>Hey ,muslim dickhead !!!!!!!! Why don;t you go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8647 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     JEW Get the fuck out of here you jewish son of...\n",
       "1     This is how I too feel about #LGBTQ #Pride It'...\n",
       "2     You're right, I didn't think about that. And w...\n",
       "3     I am transgender but I'm also mentally ill but...\n",
       "4     trump is the only jews true friend. I hope tru...\n",
       "...                                                 ...\n",
       "8642  Hitler didn't finish it. Can u. If a nigger ur...\n",
       "8643  Ah yeah, I'm actually Dominican myself! And mu...\n",
       "8644  \"@iamkrause: No need to thank me, killing nigg...\n",
       "8645  +Kinda Linda yea I know I just wish people wou...\n",
       "8646  Hey ,muslim dickhead !!!!!!!! Why don;t you go...\n",
       "\n",
       "[8647 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.tsv', delimiter='\\t', header=None, names=['text'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d6e6cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "67477966",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_annotated = pd.DataFrame(columns = ['text', 'label', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b0cb3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8647/8647 [00:10<00:00, 804.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(test_df['text']):\n",
    "    label, words = classify_identify(text)\n",
    "    row  = pd.DataFrame([[text, \n",
    "                         label,\n",
    "                         words]],\n",
    "                         columns = test_df_annotated.columns)\n",
    "    \n",
    "    test_df_annotated = pd.concat([row, test_df_annotated], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47869d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey ,muslim dickhead !!!!!!!! Why don;t you go...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[or, !, shove, allah, fuck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+Kinda Linda yea I know I just wish people wou...</td>\n",
       "      <td>non-offensive</td>\n",
       "      <td>[+kinda, see, islam, to, digging]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"@iamkrause: No need to thank me, killing nigg...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[pal, killing, dis, is, niggers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah yeah, I'm actually Dominican myself! And mu...</td>\n",
       "      <td>non-offensive</td>\n",
       "      <td>[yeah, dominican, and, myself, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hitler didn't finish it. Can u. If a nigger ur...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[in, jew, street, the, u]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          label  \\\n",
       "0  Hey ,muslim dickhead !!!!!!!! Why don;t you go...      offensive   \n",
       "1  +Kinda Linda yea I know I just wish people wou...  non-offensive   \n",
       "2  \"@iamkrause: No need to thank me, killing nigg...      offensive   \n",
       "3  Ah yeah, I'm actually Dominican myself! And mu...  non-offensive   \n",
       "4  Hitler didn't finish it. Can u. If a nigger ur...      offensive   \n",
       "\n",
       "                               words  \n",
       "0        [or, !, shove, allah, fuck]  \n",
       "1  [+kinda, see, islam, to, digging]  \n",
       "2   [pal, killing, dis, is, niggers]  \n",
       "3  [yeah, dominican, and, myself, !]  \n",
       "4          [in, jew, street, the, u]  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c9b917bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_annotated.to_csv(\"annotated_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1fa08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
